{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LVTN-FakeNews\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3002,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import networkx as nx\n",
    "from CSom import *\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=load_model('../ksoms.ckpt')\n",
    "processed_data=load_data('../data_preprocess')\n",
    "PNodes_arr=create_pnode(model, processed_data)\n",
    "\n",
    "RADIUS_MAP=16\n",
    "\n",
    "\n",
    "def build_graph_ksom():\n",
    "    graph= nx.Graph()\n",
    "\n",
    "    edge_dict={}\n",
    "    radius=1\n",
    "    edge_weighted_list=[]\n",
    "    pos={}\n",
    "    for ix, iy in np.ndindex(model.m_Som.shape):\n",
    "        idx_cnode=RADIUS_MAP*ix+iy\n",
    "        pos[idx_cnode]=(ix, iy)\n",
    "        for i in range (-radius,radius+1):\n",
    "            for j in range (0, radius+1):\n",
    "                if (i==0 and j==0):\n",
    "                    continue\n",
    "                x_idx, y_idx= ix+i, iy+j\n",
    "                if (0<=x_idx and x_idx<RADIUS_MAP and 0<=y_idx and y_idx<RADIUS_MAP):\n",
    "                    # print(f\"CNode in {ix}, {iy} near to {x_idx}, {y_idx}\")\n",
    "                    idx_cnode_neighbor=RADIUS_MAP*x_idx+y_idx\n",
    "                    if (idx_cnode,idx_cnode_neighbor) not in edge_dict and (idx_cnode_neighbor,idx_cnode) not in edge_dict:\n",
    "                        # print(\"Go here\")\n",
    "                        weights=model.m_Som[(ix, iy)].CalculateDistance2CNode(model.m_Som[x_idx, y_idx], [0.5, 0.5])\n",
    "                        # print(weights)\n",
    "                        \n",
    "                        edge_weighted_list.append([idx_cnode, idx_cnode_neighbor, round(weights,2)])\n",
    "                        edge_dict[(idx_cnode, idx_cnode_neighbor)]=1\n",
    "    # graph.add_weighted_edges_from(edge_weighted_list)\n",
    "    return graph, pos, edge_weighted_list\n",
    "\n",
    "\n",
    "def build_leaf_node_ksom(lst_weight_edge):\n",
    "    global G\n",
    "    idx_start=RADIUS_MAP*RADIUS_MAP\n",
    "    sum_quan_err=0\n",
    "    for i in range(PNodes_arr.shape[0]):\n",
    "      \n",
    "        SuitNode, ix, iy = model.FindBestMatchingNode(PNodes_arr[i])\n",
    "        weight_val= model.CalculateDistance_PNode2CNode(PNodes_arr[i], SuitNode)\n",
    "        sum_quan_err+=weight_val\n",
    "        lst_weight_edge.append([RADIUS_MAP*iy+ix, idx_start, weight_val])\n",
    "        for node_idx in model.m_Som[iy, ix].PNodes:\n",
    "            weight_pnode2pnode=model.calcdistance2PNode(model.m_Som[iy, ix].PNodes[node_idx], PNodes_arr[i])\n",
    "            lst_weight_edge.append([node_idx, idx_start, weight_pnode2pnode])\n",
    "        SuitNode.addPNode(PNodes_arr[i], idx_start)\n",
    "        idx_start+=1\n",
    "    # print(f\"Quantization Error {sum_quan_err/len()}\")\n",
    "    return lst_weight_edge\n",
    "\n",
    "\n",
    "G, pos, edge_list=build_graph_ksom()\n",
    "edge_list=build_leaf_node_ksom(edge_list)\n",
    "G.add_weighted_edges_from(edge_list)\n",
    "\n",
    "\n",
    "# labels = nx.get_edge_attributes(G,'weight')\n",
    "# print(labels)\n",
    "\n",
    "with open('ksom.nx', 'wb') as inp:\n",
    "    data=pickle.dump(G, inp)\n",
    "labels_node={}\n",
    "\n",
    "for i in range(0, 256):\n",
    "    labels_node[i]=2.0\n",
    "\n",
    "for i in range(0, len(processed_data)):\n",
    "    labels_node[RADIUS_MAP*RADIUS_MAP+i]=float(processed_data[i][-1]==True)\n",
    "values=[labels_node.get(val, 5.0) for val in G.nodes()]\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "adj = nx.to_scipy_sparse_array(G).tocoo()\n",
    "row = torch.from_numpy(adj.row.astype(np.int64)).to(torch.long)\n",
    "col = torch.from_numpy(adj.col.astype(np.int64)).to(torch.long)\n",
    "edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "labels=np.array(values).astype(np.int64)\n",
    "embeddings=[0]*(16*16+len(processed_data))\n",
    "for ix, iy in np.ndindex(model.m_Som.shape):\n",
    "    temp=tuple()\n",
    "    for w in model.m_Som[ix, iy].dWeights:\n",
    "        temp+=(w,)\n",
    "        embeddings[16*ix+iy]= np.concatenate(temp, axis=None)\n",
    "corpus=[x[0] for x in processed_data]\n",
    "PNodes = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)\n",
    "PNodes = PNodes.fit_transform(corpus).todense()\n",
    "for i in range (0, PNodes.shape[0]):\n",
    "    embeddings[16*16+i]=np.squeeze(np.asarray(PNodes[i]))\n",
    "for i in range(PNodes_arr.shape[0]):\n",
    "    temp = tuple()\n",
    "    for j in range(2):\n",
    "        temp+=(PNodes_arr[i].getvector(j),)\n",
    "    embeddings[16*16+i]=np.concatenate(temp, axis=None)\n",
    "print(embeddings[16*16+1].shape)\n",
    "embeddings=np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch_geometric.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "fake_idx = np.squeeze(np.argwhere(labels == 1))\n",
    "X_fake_train, X_fake_test = train_test_split(fake_idx, test_size=0.3, random_state=42)\n",
    "true_idx = np.squeeze(np.argwhere(labels == 0))\n",
    "X_true_train, X_true_test = train_test_split(true_idx, test_size=1-(len(X_fake_train)+random.randint(100, 200))/len(true_idx), random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.concatenate((X_true_train,X_fake_train), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(InMemoryDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super(FakeNewsDataset, self).__init__('.', transform, None, None)\n",
    "\n",
    "        data = Data(edge_index=edge_index)\n",
    "        \n",
    "        data.num_nodes = G.number_of_nodes()\n",
    "        \n",
    "        # embedding \n",
    "        data.x = torch.from_numpy(embeddings).type(torch.float32)\n",
    "        \n",
    "        # labels\n",
    "        fake_idx = np.squeeze(np.argwhere(labels == 1))\n",
    "        X_fake_train, X_fake_test = train_test_split(fake_idx, test_size=0.3, random_state=42)\n",
    "        true_idx = np.squeeze(np.argwhere(labels == 0))\n",
    "        X_true_train, X_true_test = train_test_split(true_idx, test_size=1-(len(X_fake_train)+random.randint(100, 200))/len(true_idx), random_state=50)\n",
    "        X_train=np.concatenate((X_true_train,X_fake_train), axis=None)\n",
    "        X_test=np.concatenate((X_true_test,X_fake_test), axis=None)\n",
    "        \n",
    "        for i in range(0, 256):\n",
    "            labels[i]=1\n",
    "\n",
    "        y = torch.from_numpy(labels).type(torch.long)\n",
    "        data.y = y.clone().detach()\n",
    "        \n",
    "        data.num_classes = 2\n",
    "\n",
    "        # splitting the data into train, validation and test\n",
    "        n_nodes = G.number_of_nodes()\n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[X_train] = True\n",
    "        test_mask[X_test] = True\n",
    "        data['train_mask'] = train_mask\n",
    "        data['test_mask'] = test_mask\n",
    "\n",
    "        self.data, self.slices = self.collate([data])\n",
    "\n",
    "    def _download(self):\n",
    "        return\n",
    "\n",
    "    def _process(self):\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1\n",
      "Number of features: 3002\n",
      "Number of classes: 2\n",
      "==================================================\n",
      "Data(edge_index=[2, 47684], num_nodes=2486, x=[2486, 3002], y=[2486], num_classes=2, train_mask=[2486], test_mask=[2486])\n",
      "Number of nodes: 2486\n",
      "Number of edges: 47684\n",
      "Number of training nodes: 662\n",
      "Training node label rate: 0.27\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = FakeNewsDataset()\n",
    "data = dataset[0]\n",
    "\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(50*'=')\n",
    "\n",
    "# There is only one graph in the dataset, use it as new data object\n",
    "data = dataset[0]  \n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(data)\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels_1, hidden_channels_2):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Initialize the layers\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels_1)\n",
    "        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_2)\n",
    "        self.out = GCNConv(hidden_channels_2, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Message Passing Layer (Transformation)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        # Second Message Passing Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x,p=0.25, training=self.training)\n",
    "\n",
    "        # Output layer \n",
    "        x = F.log_softmax(self.out(x, edge_index), dim=1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3002, 256)\n",
      "  (conv2): GCNConv(256, 128)\n",
      "  (out): GCNConv(128, 2)\n",
      ")\n",
      "cuda:0\n",
      "Epoch: 000, Loss: 0.6927\n",
      "Epoch: 010, Loss: 0.2760\n",
      "Epoch: 020, Loss: 0.2486\n",
      "Epoch: 030, Loss: 0.3355\n",
      "Epoch: 040, Loss: 0.2695\n",
      "Epoch: 050, Loss: 0.2494\n",
      "Epoch: 060, Loss: 0.2445\n",
      "Epoch: 070, Loss: 0.2410\n",
      "Epoch: 080, Loss: 0.2380\n",
      "Epoch: 090, Loss: 0.2437\n",
      "Epoch: 100, Loss: 0.2447\n",
      "Epoch: 110, Loss: 0.2384\n",
      "Epoch: 120, Loss: 0.2391\n",
      "Epoch: 130, Loss: 0.2480\n",
      "Epoch: 140, Loss: 0.2452\n",
      "Epoch: 150, Loss: 0.2475\n",
      "Epoch: 160, Loss: 0.2370\n",
      "Epoch: 170, Loss: 0.2735\n",
      "Epoch: 180, Loss: 0.2548\n",
      "Epoch: 190, Loss: 0.2501\n",
      "Epoch: 200, Loss: 0.2399\n",
      "Epoch: 210, Loss: 0.2364\n",
      "Epoch: 220, Loss: 0.2354\n",
      "Epoch: 230, Loss: 0.2388\n",
      "Epoch: 240, Loss: 0.2485\n",
      "Epoch: 250, Loss: 0.2395\n",
      "Epoch: 260, Loss: 0.2365\n",
      "Epoch: 270, Loss: 0.2377\n",
      "Epoch: 280, Loss: 0.2380\n",
      "Epoch: 290, Loss: 0.2354\n",
      "Epoch: 300, Loss: 0.2390\n",
      "Epoch: 310, Loss: 0.2369\n",
      "Epoch: 320, Loss: 0.2466\n",
      "Epoch: 330, Loss: 0.2401\n",
      "Epoch: 340, Loss: 0.2372\n",
      "Epoch: 350, Loss: 0.2361\n",
      "Epoch: 360, Loss: 0.2361\n",
      "Epoch: 370, Loss: 0.2359\n",
      "Epoch: 380, Loss: 0.2355\n",
      "Epoch: 390, Loss: 0.2387\n",
      "Epoch: 400, Loss: 0.2382\n",
      "Epoch: 410, Loss: 0.2363\n",
      "Epoch: 420, Loss: 0.2368\n",
      "Epoch: 430, Loss: 0.2381\n",
      "Epoch: 440, Loss: 0.2447\n",
      "Epoch: 450, Loss: 0.2354\n",
      "Epoch: 460, Loss: 0.2364\n",
      "Epoch: 470, Loss: 0.2354\n",
      "Epoch: 480, Loss: 0.2353\n",
      "Epoch: 490, Loss: 0.2359\n",
      "Epoch: 500, Loss: 0.2380\n",
      "Epoch: 510, Loss: 0.2369\n",
      "Epoch: 520, Loss: 0.2361\n",
      "Epoch: 530, Loss: 0.2394\n",
      "Epoch: 540, Loss: 0.2361\n",
      "Epoch: 550, Loss: 0.2359\n",
      "Epoch: 560, Loss: 0.2354\n",
      "Epoch: 570, Loss: 0.2356\n",
      "Epoch: 580, Loss: 0.2346\n",
      "Epoch: 590, Loss: 0.2353\n",
      "Epoch: 600, Loss: 0.2560\n",
      "Epoch: 610, Loss: 0.2442\n",
      "Epoch: 620, Loss: 0.2356\n",
      "Epoch: 630, Loss: 0.2357\n",
      "Epoch: 640, Loss: 0.2361\n",
      "Epoch: 650, Loss: 0.2356\n",
      "Epoch: 660, Loss: 0.2352\n",
      "Epoch: 670, Loss: 0.2354\n",
      "Epoch: 680, Loss: 0.2351\n",
      "Epoch: 690, Loss: 0.2370\n",
      "Epoch: 700, Loss: 0.2355\n",
      "Epoch: 710, Loss: 0.2359\n",
      "Epoch: 720, Loss: 0.2378\n",
      "Epoch: 730, Loss: 0.2364\n",
      "Epoch: 740, Loss: 0.2383\n",
      "Epoch: 750, Loss: 0.2354\n",
      "Epoch: 760, Loss: 0.2359\n",
      "Epoch: 770, Loss: 0.2360\n",
      "Epoch: 780, Loss: 0.2356\n",
      "Epoch: 790, Loss: 0.2386\n",
      "Epoch: 800, Loss: 0.2402\n",
      "Epoch: 810, Loss: 0.2375\n",
      "Epoch: 820, Loss: 0.2347\n",
      "Epoch: 830, Loss: 0.2429\n",
      "Epoch: 840, Loss: 0.2415\n",
      "Epoch: 850, Loss: 0.2358\n",
      "Epoch: 860, Loss: 0.2363\n",
      "Epoch: 870, Loss: 0.2424\n",
      "Epoch: 880, Loss: 0.2408\n",
      "Epoch: 890, Loss: 0.2386\n",
      "Epoch: 900, Loss: 0.2377\n",
      "Epoch: 910, Loss: 0.2349\n",
      "Epoch: 920, Loss: 0.2347\n",
      "Epoch: 930, Loss: 0.2350\n",
      "Epoch: 940, Loss: 0.2359\n",
      "Epoch: 950, Loss: 0.2472\n",
      "Epoch: 960, Loss: 0.2355\n",
      "Epoch: 970, Loss: 0.2514\n",
      "Epoch: 980, Loss: 0.2461\n",
      "Epoch: 990, Loss: 0.2372\n",
      "Epoch: 1000, Loss: 0.2363\n",
      "Epoch: 1010, Loss: 0.2353\n",
      "Epoch: 1020, Loss: 0.2354\n",
      "Epoch: 1030, Loss: 0.2363\n",
      "Epoch: 1040, Loss: 0.2353\n",
      "Epoch: 1050, Loss: 0.2351\n",
      "Epoch: 1060, Loss: 0.2345\n",
      "Epoch: 1070, Loss: 0.2359\n",
      "Epoch: 1080, Loss: 0.2351\n",
      "Epoch: 1090, Loss: 0.2346\n",
      "Epoch: 1100, Loss: 0.2357\n",
      "Epoch: 1110, Loss: 0.2369\n",
      "Epoch: 1120, Loss: 0.2428\n",
      "Epoch: 1130, Loss: 0.2358\n",
      "Epoch: 1140, Loss: 0.2350\n",
      "Epoch: 1150, Loss: 0.2354\n",
      "Epoch: 1160, Loss: 0.2349\n",
      "Epoch: 1170, Loss: 0.2358\n",
      "Epoch: 1180, Loss: 0.2448\n",
      "Epoch: 1190, Loss: 0.2357\n",
      "Epoch: 1200, Loss: 0.2362\n",
      "Epoch: 1210, Loss: 0.2377\n",
      "Epoch: 1220, Loss: 0.2353\n",
      "Epoch: 1230, Loss: 0.2359\n",
      "Epoch: 1240, Loss: 0.2355\n",
      "Epoch: 1250, Loss: 0.2401\n",
      "Epoch: 1260, Loss: 0.2371\n",
      "Epoch: 1270, Loss: 0.2358\n",
      "Epoch: 1280, Loss: 0.2341\n",
      "Epoch: 1290, Loss: 0.2355\n",
      "Epoch: 1300, Loss: 0.2360\n",
      "Epoch: 1310, Loss: 0.2364\n",
      "Epoch: 1320, Loss: 0.2368\n",
      "Epoch: 1330, Loss: 0.2372\n",
      "Epoch: 1340, Loss: 0.2363\n",
      "Epoch: 1350, Loss: 0.2351\n",
      "Epoch: 1360, Loss: 0.2370\n",
      "Epoch: 1370, Loss: 0.2389\n",
      "Epoch: 1380, Loss: 0.2408\n",
      "Epoch: 1390, Loss: 0.2385\n",
      "Epoch: 1400, Loss: 0.2349\n",
      "Epoch: 1410, Loss: 0.2349\n",
      "Epoch: 1420, Loss: 0.2354\n",
      "Epoch: 1430, Loss: 0.2347\n",
      "Epoch: 1440, Loss: 0.2355\n",
      "Epoch: 1450, Loss: 0.2369\n",
      "Epoch: 1460, Loss: 0.2362\n",
      "Epoch: 1470, Loss: 0.2359\n",
      "Epoch: 1480, Loss: 0.2368\n",
      "Epoch: 1490, Loss: 0.2359\n",
      "Epoch: 1500, Loss: 0.2369\n",
      "Epoch: 1510, Loss: 0.2360\n",
      "Epoch: 1520, Loss: 0.2352\n",
      "Epoch: 1530, Loss: 0.2360\n",
      "Epoch: 1540, Loss: 0.2349\n",
      "Epoch: 1550, Loss: 0.2355\n",
      "Epoch: 1560, Loss: 0.2352\n",
      "Epoch: 1570, Loss: 0.2367\n",
      "Epoch: 1580, Loss: 0.2396\n",
      "Epoch: 1590, Loss: 0.2355\n",
      "Epoch: 1600, Loss: 0.2365\n",
      "Epoch: 1610, Loss: 0.2374\n",
      "Epoch: 1620, Loss: 0.2361\n",
      "Epoch: 1630, Loss: 0.2352\n",
      "Epoch: 1640, Loss: 0.2339\n",
      "Epoch: 1650, Loss: 0.2340\n",
      "Epoch: 1660, Loss: 0.2350\n",
      "Epoch: 1670, Loss: 0.2356\n",
      "Epoch: 1680, Loss: 0.2365\n",
      "Epoch: 1690, Loss: 0.2360\n",
      "Epoch: 1700, Loss: 0.2363\n",
      "Epoch: 1710, Loss: 0.2351\n",
      "Epoch: 1720, Loss: 0.2385\n",
      "Epoch: 1730, Loss: 0.2355\n",
      "Epoch: 1740, Loss: 0.2353\n",
      "Epoch: 1750, Loss: 0.2353\n",
      "Epoch: 1760, Loss: 0.2387\n",
      "Epoch: 1770, Loss: 0.2390\n",
      "Epoch: 1780, Loss: 0.2358\n",
      "Epoch: 1790, Loss: 0.2358\n",
      "Epoch: 1800, Loss: 0.2361\n",
      "Epoch: 1810, Loss: 0.2360\n",
      "Epoch: 1820, Loss: 0.2352\n",
      "Epoch: 1830, Loss: 0.2350\n",
      "Epoch: 1840, Loss: 0.2375\n",
      "Epoch: 1850, Loss: 0.2374\n",
      "Epoch: 1860, Loss: 0.2346\n",
      "Epoch: 1870, Loss: 0.2345\n",
      "Epoch: 1880, Loss: 0.2363\n",
      "Epoch: 1890, Loss: 0.2348\n",
      "Epoch: 1900, Loss: 0.2426\n",
      "Epoch: 1910, Loss: 0.2352\n",
      "Epoch: 1920, Loss: 0.2367\n",
      "Epoch: 1930, Loss: 0.2395\n",
      "Epoch: 1940, Loss: 0.2465\n",
      "Epoch: 1950, Loss: 0.2372\n",
      "Epoch: 1960, Loss: 0.2347\n",
      "Epoch: 1970, Loss: 0.2386\n",
      "Epoch: 1980, Loss: 0.2351\n",
      "Epoch: 1990, Loss: 0.2381\n",
      "torch.Size([2486, 2])\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels_1=256, hidden_channels_2=128)\n",
    "print(model)\n",
    "# Use GPU\n",
    "print(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "# Define loss function (CrossEntropyLoss for Classification Problems with \n",
    "# probability distributions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      # Use all data as input, because all nodes have node features\n",
    "      out = model(data.x, data.edge_index)  \n",
    "      # Only use nodes with labels available for loss calculation --> mask\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  \n",
    "      loss.backward() \n",
    "      optimizer.step()\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      # Use the class with highest probability.\n",
    "      pred = out.argmax(dim=1)\n",
    "      # Check against ground-truth labels.\n",
    "\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "      # Derive ratio of correct predictions.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "      train_correct = pred[data.train_mask] == data.y[data.train_mask]  \n",
    "      # Derive ratio of correct predictions.\n",
    "      train_acc = int(train_correct.sum()) / int(data.train_mask.sum())  \n",
    "      return test_acc, train_acc\n",
    "      \n",
    "\n",
    "losses = []\n",
    "for epoch in range(0, 2000):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "print(model(data.x, data.edge_index).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 18 57 412\n",
      "Accuracy 0.8867069486404834\n",
      "Precision 0.9067357512953368\n",
      "Recall 0.7543103448275862\n"
     ]
    }
   ],
   "source": [
    "pred = model(data.x, data.edge_index)\n",
    "\n",
    "TP =0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "for i in range (256, 2486):\n",
    "  pred_labels=np.argmax(pred[i].detach().cpu().numpy())\n",
    "  if data.test_mask[i]==False:\n",
    "    if data.y[i]==1 and pred_labels==1:\n",
    "      TP+=1\n",
    "    elif data.y[i]==1 and pred_labels==0:\n",
    "      FN+=1\n",
    "    elif data.y[i]==0 and pred_labels==0:\n",
    "      TN+=1\n",
    "    else:\n",
    "      FP+=1\n",
    "print(TP, FP, FN, TN)\n",
    "print(f\"Accuracy {(TP+TN)/(FP+FN+TP+TN)}\")\n",
    "print(f\"Precision {(TP)/(TP+FP)}\")\n",
    "print(f\"Recall {(TP)/(TP+FN)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1907b72c9494c0dab9cb4c8813bd73f54c44b33596584365c9f57b07d01e2ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
